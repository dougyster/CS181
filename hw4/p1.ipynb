{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CS 181 HW4 Problem 1**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize data and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a specific example of when we have $K = 3$ component Gamma distributions. Let's initialize the initial parameter values for $\\theta$ and $\\beta_k$ as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\theta_k &=  1/K, \\\\\n",
    "  \\beta_k & = k/K.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note that we usually initialize $\\theta$ and $\\beta_k$ randomly. However, by fixing the initial $\\theta$ and $\\beta_k$, EM becomes deterministic which makes debugging (and grading) easier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as ds\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "x = torch.load('data.pt')\n",
    "\n",
    "K = 3\n",
    "theta = torch.ones(K) / K\n",
    "alpha = 5.0\n",
    "betas = (torch.arange(K) + 1) / K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import gamma\n",
    "x = x.numpy()\n",
    "theta = theta.numpy()\n",
    "betas = betas.numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Todo:** implement the E-step (Use expression derived in part 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def e_step(theta, betas):\n",
    "    num_data_points = len(x)\n",
    "    num_components = len(betas)\n",
    "    \n",
    "    q = np.zeros((num_data_points, num_components))\n",
    "\n",
    "    for n in range(num_data_points):\n",
    "        for k in range(num_components):\n",
    "            likelihood = gamma.pdf(x[n], alpha, scale=betas[k])\n",
    "            q[n, k] = likelihood * theta[k]\n",
    "\n",
    "    q /= q.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return q\n",
    "\n",
    "type(betas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Todo:** implement the M-step (Use expressions derived in parts 4b and 4c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(q):\n",
    "    theta_hat = (q.sum(axis = 1)).sum(axis = 0)/ len(q)\n",
    "    \n",
    "    beta_hats = 0\n",
    "    temp = 0\n",
    "    for i in range(len(q)):\n",
    "        beta_hats += q[i] * x[i]\n",
    "        temp += q[i] * alpha\n",
    "        \n",
    "    return theta_hat, (beta_hats/temp).sum(axis = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log Likelihood Implemented Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "def log_px(x, theta, betas):\n",
    "    log_pxs = gamma.logpdf(x, alpha, scale=1/betas) + np.log(theta)\n",
    "    \n",
    "    return logsumexp(log_pxs, axis=1)\n",
    "\n",
    "def log_likelihood(theta, betas):\n",
    "    return log_px(x, theta, betas).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Todo:** implement EM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_em(theta, betas, iterations=1000):\n",
    "\n",
    "    prev_log_likelihood = log_likelihood(theta, betas)\n",
    "\n",
    "    for i in range(0,iterations):\n",
    "\n",
    "        q = e_step(theta, betas)\n",
    "        theta, betas = m_step(q)\n",
    "\n",
    "        new_log_likelihood = log_likelihood(theta, betas)\n",
    "\n",
    "        if abs(new_log_likelihood - prev_log_likelihood) < 1e-6:\n",
    "            print(\"EM algorithm converged \")\n",
    "            break\n",
    "\n",
    "        prev_log_likelihood = new_log_likelihood\n",
    "\n",
    "    return theta, betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(theta, betas):\n",
    "    x_test = torch.linspace(0.01, x.max(), 1000)\n",
    "    prob = log_px(x_test.unsqueeze(-1), theta, betas).exp()\n",
    "    # prob = np.exp(log_px(x_test.unsqueeze(-1), theta, betas))  # use this line for numpy\n",
    "    ll = log_likelihood(theta, betas)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    \n",
    "    fig.subplots_adjust(top=0.7)\n",
    "    fig.suptitle(f'theta = {theta}\\nbeta = {betas}\\nlog likelihood = {ll:.3e}')\n",
    "\n",
    "    ax1.set_title('Dataset')\n",
    "    ax1.hist(x.T, bins=100, color='tomato')\n",
    "    # ax1.hist(x, bins=100, color='tomato')  # use this line for numpy\n",
    "    ax2.set_title('Gamma mixture')\n",
    "    ax2.plot(x_test, prob, color='tomato')\n",
    "    plt.savefig('p1.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m theta, betas \u001b[39m=\u001b[39m run_em(theta, betas)\n\u001b[1;32m      2\u001b[0m make_plot(theta, betas)\n",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m, in \u001b[0;36mrun_em\u001b[0;34m(theta, betas, iterations)\u001b[0m\n\u001b[1;32m      3\u001b[0m prev_log_likelihood \u001b[39m=\u001b[39m log_likelihood(theta, betas)\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,iterations):\n\u001b[0;32m----> 7\u001b[0m     q \u001b[39m=\u001b[39m e_step(theta, betas)\n\u001b[1;32m      8\u001b[0m     theta, betas \u001b[39m=\u001b[39m m_step(q)\n\u001b[1;32m     10\u001b[0m     new_log_likelihood \u001b[39m=\u001b[39m log_likelihood(theta, betas)\n",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m, in \u001b[0;36me_step\u001b[0;34m(theta, betas)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39me_step\u001b[39m(theta, betas):\n\u001b[1;32m      2\u001b[0m     num_data_points \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(x)\n\u001b[0;32m----> 3\u001b[0m     num_components \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39;49m(betas)\n\u001b[1;32m      5\u001b[0m     q \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((num_data_points, num_components))\n\u001b[1;32m      7\u001b[0m     \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_data_points):\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "theta, betas = run_em(theta, betas)\n",
    "make_plot(theta, betas)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c365ea26a318e0b540d1978e97e3d03cfe51dff8cd04dae5f3d7b47d79d2453"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
